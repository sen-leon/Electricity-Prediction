{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive RNN with continuous input\n",
    "- This model is autoregressive, which means that the prediction of the last time step is the input of the current time step. This way the model can predict a varying number of steps into the future without retraining\n",
    "- Some features, like time and weather, are fed into the model from outside even during prediction phase so the model does not have to predict those by itself\n",
    "\n",
    "Based on: https://www.tensorflow.org/tutorials/structured_data/time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import IPython\n",
    "import IPython.display\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle(\"..\\data\\data_prepared.pkl\")\n",
    "df = pd.read_pickle(\"../data/prepared/electricity+time_2017-2021.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data\n",
    "- 70% training\n",
    "- 20% validation\n",
    "- 10% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_indices = {name: i for i, name in enumerate(df.columns)}\n",
    "\n",
    "n = len(df)\n",
    "df_train = df[0:int(n*0.7)]\n",
    "df_val = df[int(n*0.7):int(n*0.9)]\n",
    "df_test = df[int(n*0.9):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# fit scaler to training data\n",
    "scaler.fit(df_train)\n",
    "# scale all sets according to train set, preserve data frames\n",
    "df_train = pd.DataFrame(scaler.transform(df_train),\n",
    "                        columns=df.columns, index=df_train.index)\n",
    "df_val = pd.DataFrame(scaler.transform(df_val),\n",
    "                        columns=df.columns, index=df_val.index)\n",
    "df_test = pd.DataFrame(scaler.transform(df_test),\n",
    "                        columns=df.columns, index=df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Windowing\n",
    "#### 1. Indexes and offsets\n",
    "\n",
    "- Windows Generator that can create multiple inputs, one for the inputs that are only known in the past (power production) and one for inputs that are also known in the future (time and weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, past_width, future_width,\n",
    "                 label_columns, precise_columns=None, forecast_columns=None,\n",
    "                 train_df=df_train, val_df=df_val, test_df=df_test):\n",
    "\n",
    "        # Store the raw data.\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "\n",
    "        # Check if length of provided features match with data frame\n",
    "        num_columns = 0\n",
    "        num_columns += len(label_columns)\n",
    "        if precise_columns is not None:\n",
    "            num_columns += len(precise_columns)\n",
    "        if forecast_columns is not None:\n",
    "            num_columns += len(forecast_columns)\n",
    "        assert num_columns == len(train_df.columns), \\\n",
    "            \"Length of provided label, precise, and forecast features do not match data frame\"\n",
    "\n",
    "        # Work out the column indices\n",
    "        self.columns_indices = {name: i for i, name in\n",
    "                                enumerate(train_df.columns)}\n",
    "        self.label_columns = label_columns\n",
    "        self.label_columns_indices = {name: i for i, name in\n",
    "                                      enumerate(label_columns)}\n",
    "        self.precise_columns = precise_columns\n",
    "        self.forecast_columns = forecast_columns\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.past_width = past_width\n",
    "        self.future_width = future_width\n",
    "\n",
    "        self.total_window_size = past_width + future_width\n",
    "\n",
    "        self.input_past_slice = slice(0, past_width)\n",
    "        self.input_past_indices = np.arange(self.total_window_size)[\n",
    "            self.input_past_slice]\n",
    "\n",
    "        self.input_future_slice = slice(self.past_width, -1)\n",
    "        self.input_future_indices = np.arange(self.total_window_size)[\n",
    "            self.input_future_slice]\n",
    "\n",
    "        self.label_slice = slice(self.past_width, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[\n",
    "            self.label_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size:    {self.total_window_size}',\n",
    "            f'Input past indices:   {self.input_past_indices}',\n",
    "            f'Input future indices: {self.input_future_indices}',\n",
    "            f'Label indices:        {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "        # f'Precise column name(s):  {self.precise_columns}',\n",
    "        # f'Forecast column name(s): {self.forecast_columns}'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_window(self, features):\n",
    "    # Past: All inputs are known\n",
    "    inputs_past = features[:, self.input_past_slice, :]\n",
    "\n",
    "    # Future: Only precise and forecast inputs are known\n",
    "    if self.precise_columns is None:\n",
    "        self.precise_columns = []\n",
    "    if self.forecast_columns is None:\n",
    "        self.forecast_columns = []\n",
    "    self.input_future_columns = self.precise_columns + self.forecast_columns\n",
    "\n",
    "    inputs_future = tf.stack(\n",
    "        [features[:, self.input_future_slice, self.columns_indices[name]]\n",
    "            for name in self.input_future_columns],\n",
    "        axis=-1)\n",
    "\n",
    "    labels = tf.stack(\n",
    "        [features[:, self.label_slice, self.columns_indices[name]]\n",
    "            for name in self.label_columns],\n",
    "        axis=-1)\n",
    "\n",
    "    # Slicing doesn't preserve static shape information, so set the shapes\n",
    "    # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "    inputs_past.set_shape([None, self.past_width, None])\n",
    "    inputs_future.set_shape([None, self.future_width-1, None])\n",
    "    labels.set_shape([None, self.future_width, None])\n",
    "\n",
    "    # Return inputs and labels\n",
    "    # The past and future input tuple will be unpacked in the model.call() method\n",
    "    return (inputs_past, inputs_future), labels\n",
    "\n",
    "\n",
    "WindowGenerator.split_window = split_window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(self, model=None, plot_col=\"prod_solar\", max_subplots=3):\n",
    "    inputs, labels = self.example\n",
    "    (inputs_past, inputs_future) = inputs\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plot_col_index = self.columns_indices[plot_col]\n",
    "    max_n = min(max_subplots, len(inputs_past))\n",
    "    for n in range(max_n):\n",
    "        plt.subplot(max_n, 1, n+1)\n",
    "\n",
    "        # Plot past inputs\n",
    "        plt.plot(self.input_past_indices, inputs_past[n, :, plot_col_index],\n",
    "                 label='Inputs', marker='.', zorder=3)\n",
    "\n",
    "        label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "\n",
    "        # Add last point of past array to future arrays to connect lines\n",
    "        label_indices_plot = np.insert(\n",
    "            self.label_indices, 0, self.input_past_indices[-1])\n",
    "        labels_plot = np.insert(\n",
    "            labels[n, :, label_col_index], 0, inputs_past[n, :, plot_col_index][-1])\n",
    "\n",
    "        # Plot labels\n",
    "        plt.plot(label_indices_plot, labels_plot,\n",
    "                 'C2', label='Labels', marker='.', zorder=1)\n",
    "\n",
    "        # Plot prediction\n",
    "        if model is not None:\n",
    "            predictions_plot = (model(inputs))[n, :, label_col_index]\n",
    "            predictions_plot = np.insert(\n",
    "                predictions_plot, 0, inputs_past[n, :, plot_col_index][-1])\n",
    "            plt.plot(label_indices_plot, predictions_plot,\n",
    "                     'C1', label='Predictions', marker='.', zorder=2)\n",
    "\n",
    "        # x Ticks every 6 hours\n",
    "        plt.xticks(np.arange(self.input_past_indices[0], self.label_indices[-1]+2, 6))\n",
    "\n",
    "        plt.ylabel(f'{plot_col} [normed]')\n",
    "        if n == 0:\n",
    "            plt.legend()\n",
    "\n",
    "    plt.xlabel('Time [h]')\n",
    "\n",
    "\n",
    "WindowGenerator.plot = plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Create `tf.data.Datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(self, data):\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        data=data,\n",
    "        targets=None,\n",
    "        sequence_length=self.total_window_size,\n",
    "        sequence_stride=1,\n",
    "        shuffle=True,\n",
    "        batch_size=32,)\n",
    "\n",
    "    ds = ds.map(self.split_window)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "WindowGenerator.make_dataset = make_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@property\n",
    "def train(self):\n",
    "    return self.make_dataset(self.train_df)\n",
    "\n",
    "\n",
    "@property\n",
    "def val(self):\n",
    "    return self.make_dataset(self.val_df)\n",
    "\n",
    "\n",
    "@property\n",
    "def test(self):\n",
    "    return self.make_dataset(self.test_df)\n",
    "\n",
    "\n",
    "@property\n",
    "def example(self):\n",
    "    \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "    result = getattr(self, '_example', None)\n",
    "    if result is None:\n",
    "        # No example batch was found, so get one from the `.test` dataset\n",
    "        result = next(iter(self.test))\n",
    "        # And cache it for next time\n",
    "        self._example = result\n",
    "    return result\n",
    "\n",
    "\n",
    "WindowGenerator.train = train\n",
    "WindowGenerator.val = val\n",
    "WindowGenerator.test = test\n",
    "WindowGenerator.example = example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which features are only known for the past. These features will be predicted.\n",
    "prediction_features = ['prod_wind', 'prod_solar']\n",
    "num_predictions = len(prediction_features)\n",
    "\n",
    "# Choose which features are precisely known to the model at all time steps\n",
    "precise_features = ['day_sin', 'day_cos', 'year_sin',\n",
    "                    'year_cos', 'solar_el', 'solar_el_clip']\n",
    "\n",
    "# Choose which features are only imprecisely known to the model at future time steps\n",
    "# NOISE NOT YET IMPLEMENTED\n",
    "forecast_features = None\n",
    "\n",
    "PAST_STEPS = 24\n",
    "PREDICTION_STEPS = 12\n",
    "\n",
    "multi_window = WindowGenerator(\n",
    "    past_width=PAST_STEPS, future_width=PREDICTION_STEPS,\n",
    "    label_columns=prediction_features,\n",
    "    precise_columns=precise_features,\n",
    "    forecast_columns=forecast_features\n",
    ")\n",
    "\n",
    "multi_window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the `WindowGenerator` object gives access to the tf.data.Dataset objects, to easily iterate over the data.\n",
    "\n",
    "The `Dataset.element_spec` property tells the structure, data types, and shapes of the dataset elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_window.train.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating over a `Dataset` yields concrete batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (example_inputs_past, example_inputs_future), example_labels in multi_window.train.take(1):\n",
    "  print(f'Inputs past shape (batch, time, features): {example_inputs_past.shape}')\n",
    "  print(f'Inputs fut. shape (batch, time, features): {example_inputs_future.shape}')\n",
    "  print(f'Labels shape      (batch, time, features): {example_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot an example window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_window.plot(plot_col=\"prod_solar\")\n",
    "# multi_window.plot(plot_col=\"prod_wind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive LSTM\n",
    "The model predicts individual time steps that are fed back into itself, so that the model can produce output with a varying length.\n",
    "This is implemented in the custom model class `FeedBack`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedBack(tf.keras.Model):\n",
    "    def __init__(self, units, out_steps):\n",
    "        super().__init__()\n",
    "        self.out_steps = out_steps\n",
    "        self.units = units\n",
    "        self.rnn_cell = tf.keras.layers.LSTMCell(units)\n",
    "        # Also wrap the LSTMCell in an RNN to simplify the `warmup` method.\n",
    "        self.rnn_layer = tf.keras.layers.RNN(self.rnn_cell, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(num_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arlstm_model = FeedBack(units=32, out_steps=PREDICTION_STEPS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model warmup\n",
    "The warmup method initializes the model's internal state based on the full input from the past (power data, time and weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup(self, inputs_past):\n",
    "    # inputs.shape => (batch, time, features)\n",
    "    # x.shape => (batch, lstm_units)\n",
    "    x, *state = self.rnn_layer(inputs_past)\n",
    "\n",
    "    # predictions.shape => (batch, features)\n",
    "    prediction = self.dense(x)\n",
    "    return prediction, state\n",
    "\n",
    "\n",
    "FeedBack.warmup = warmup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method returns a single time-step prediction and the internal state of the `LSTM`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, state = arlstm_model.warmup(multi_window.example[0][0])\n",
    "prediction.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model call\n",
    "A custom call is used to feed the model the full input from the past during warmup and then only the time and weather forecast during the prediction phase, where the model takes the prediction from the last time step to substitute the missing input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(self, inputs, training=None):\n",
    "    # unpack the past and future inputs\n",
    "    inputs_past, inputs_future = inputs\n",
    "    # Use a TensorArray to capture dynamically unrolled outputs.\n",
    "    predictions = []\n",
    "    # Past phase: Initialize the LSTM state\n",
    "    prediction, state = self.warmup(inputs_past)\n",
    "    # Insert the first prediction.\n",
    "    predictions.append(prediction)\n",
    "\n",
    "    # Future phase: Run the rest of the prediction steps\n",
    "    for n in range(self.out_steps-1):\n",
    "        # Select the known input at the current time step\n",
    "        input = inputs_future[:, n, :]\n",
    "        # Use the last prediction as unknown input and combine it with the known input\n",
    "        # x.shape => (batch, features)\n",
    "        x = tf.concat([prediction, input], axis=1)\n",
    "        # Execute one lstm step.\n",
    "        x, state = self.rnn_cell(x, states=state,\n",
    "                                  training=training)\n",
    "        # Convert the lstm output to a prediction.\n",
    "        prediction = self.dense(x)\n",
    "        # Add the prediction to the output.\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    # predictions.shape => (time, batch, features)\n",
    "    predictions = tf.stack(predictions)\n",
    "    # predictions.shape => (batch, time, features)\n",
    "    predictions = tf.transpose(predictions, [1, 0, 2])\n",
    "    return predictions\n",
    "\n",
    "\n",
    "FeedBack.call = call\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on example input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Output shape (batch, time, features): ',\n",
    "      arlstm_model(multi_window.example[0]).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arlstm_model.call(multi_window.example[0])\n",
    "arlstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.metrics.r_square import RSquare\n",
    "import os\n",
    "\n",
    "def compile_and_fit(model, window, epochs=100, save=False, stop_early=True, model_name=None, patience=10, verbose='auto'):\n",
    "    callbacks = []\n",
    "    if save:\n",
    "        assert model_name is not None, \"No model name provided\"\n",
    "        checkpoint_folder = '../models/'+model_name\n",
    "        if not os.path.exists(checkpoint_folder):\n",
    "            os.makedirs(checkpoint_folder)\n",
    "        model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_folder+'/ckp-weights.hdf5',\n",
    "            save_weights_only=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_best_only=True)\n",
    "        callbacks.append(model_checkpoint)\n",
    "    if stop_early:\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                          patience=patience,\n",
    "                                                          mode='min',\n",
    "                                                          verbose=1,\n",
    "                                                          restore_best_weights=True)\n",
    "        callbacks.append(early_stopping)\n",
    "\n",
    "    model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=5e-4), metrics=['mae', RSquare()])\n",
    "\n",
    "    history = model.fit(window.train, epochs=epochs,\n",
    "                        validation_data=window.val,\n",
    "                        callbacks=callbacks,\n",
    "                        verbose=verbose)\n",
    "    return history\n",
    "\n",
    "\n",
    "val_performance = {}\n",
    "test_performance = {}\n",
    "history = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ARLSTM 32'\n",
    "history[model_name] = compile_and_fit(arlstm_model, multi_window, save=True, model_name='ARLSTM32')\n",
    "IPython.display.clear_output()\n",
    "\n",
    "val_performance[model_name] = arlstm_model.evaluate(multi_window.val, verbose=0)\n",
    "test_performance[model_name] = arlstm_model.evaluate(multi_window.test, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_window.plot(arlstm_model, plot_col=\"prod_wind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Training Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history['ARLSTM 32'].history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(history):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(history.history['loss'], label='Training loss (MSE)')\n",
    "    plt.plot(history.history['val_loss'], label='Validation loss (MSE)')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend(loc='upper left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss(history['ARLSTM 32'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive LSTM with Weather Data\n",
    "- RNN with one LSTM layer with 32 units\n",
    "\n",
    "Import weather data and add to data frame. Each pickle contains one weather parameter measured at multiple stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features that will be extracted from the EM data and their shorter handle\n",
    "wx_params = {\n",
    "    \"temperature_air_mean_200\": \"temp\",\n",
    "    \"wind_speed\": \"wind\",\n",
    "    \"pressure_air_site\": \"press\",\n",
    "    \"precipitation_height\": \"precip\",\n",
    "    \"sunshine_duration\": \"sun\"\n",
    "}\n",
    "\n",
    "# Concatenate all weather parameters into one data frame\n",
    "wx_list = []\n",
    "for name, handle in wx_params.items():\n",
    "    # wx_list.append(pd.read_pickle(\"../data/weather/cleaned/\"+name+\".pkl\").add_prefix(handle+\"_\"))\n",
    "    wx_list.append(pd.read_pickle(\"../data/weather/long/cleaned/\"+name+\".pkl\").add_prefix(handle+\"_\"))\n",
    "\n",
    "wx = pd.concat(wx_list, axis=1, ignore_index=False, verify_integrity=True)\n",
    "wx.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine weather data with previous Electricity Map and Time data frame, split and normalize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df, wx], axis=1, ignore_index=False, verify_integrity=True)\n",
    "\n",
    "column_indices = {name: i for i, name in enumerate(df2.columns)}\n",
    "\n",
    "n = len(df2)\n",
    "df2_train = df2[0:int(n*0.7)]\n",
    "df2_val = df2[int(n*0.7):int(n*0.9)]\n",
    "df2_test = df2[int(n*0.9):]\n",
    "\n",
    "scaler2 = RobustScaler()\n",
    "# fit scaler to training data\n",
    "scaler2.fit(df2_train)\n",
    "# scale all sets according to train set, preserve data frames\n",
    "df2_train = pd.DataFrame(scaler2.transform(df2_train),\n",
    "                        columns=df2.columns, index=df2_train.index)\n",
    "df2_val = pd.DataFrame(scaler2.transform(df2_val),\n",
    "                        columns=df2.columns, index=df2_val.index)\n",
    "df2_test = pd.DataFrame(scaler2.transform(df2_test),\n",
    "                        columns=df2.columns, index=df2_test.index)\n",
    "\n",
    "len(df2_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new window that includes weather data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wx.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which features are only known for the past. These features will be predicted.\n",
    "# prediction_features same as before\n",
    "\n",
    "# Choose which features are precisely known to the model at all time steps\n",
    "# precise_features same as before\n",
    "\n",
    "# Choose which features are only imprecisely known to the model at future time steps\n",
    "# NOISE NOT YET IMPLEMENTED\n",
    "forecast_features = list(wx.columns)\n",
    "\n",
    "PAST_STEPS = 24\n",
    "PREDICTION_STEPS = 12\n",
    "\n",
    "weather_window = WindowGenerator(\n",
    "    past_width=PAST_STEPS, future_width=PREDICTION_STEPS,\n",
    "    label_columns=prediction_features,\n",
    "    precise_columns=precise_features,\n",
    "    forecast_columns=forecast_features,\n",
    "    train_df=df2_train, val_df=df2_val, test_df=df2_test\n",
    ")\n",
    "\n",
    "weather_window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the structure of the newly created `weather_window`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_window.train.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arlstm_wx_model = FeedBack(units=32, out_steps=PREDICTION_STEPS)\n",
    "\n",
    "# Run on example input\n",
    "print('Output shape (batch, time, features): ',\n",
    "      arlstm_wx_model(weather_window.example[0]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ARLSTM 32 + wx'\n",
    "\n",
    "history[model_name] = compile_and_fit(arlstm_wx_model, weather_window)\n",
    "IPython.display.clear_output()\n",
    "\n",
    "val_performance[model_name] = arlstm_wx_model.evaluate(weather_window.val, verbose=0)\n",
    "test_performance[model_name] = arlstm_wx_model.evaluate(weather_window.test, verbose=1)\n",
    "\n",
    "model_loss(history[model_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_window.plot(arlstm_wx_model, plot_col=\"prod_wind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive GRU with Weather Data\n",
    "- RNN with one GRU layer with 32 units\n",
    "\n",
    "Modify the `FeedBack` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedBackGRU(FeedBack):\n",
    "    def __init__(self, units, out_steps):\n",
    "        super().__init__(units, out_steps)\n",
    "        self.out_steps = out_steps\n",
    "        self.units = units\n",
    "        self.lstm_cell = tf.keras.layers.GRUCell(units)\n",
    "        # Also wrap the LSTMCell in an RNN to simplify the `warmup` method.\n",
    "        self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(num_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ARGRU 32 + wx'\n",
    "\n",
    "argru_wx_model = FeedBackGRU(units=32, out_steps=PREDICTION_STEPS)\n",
    "\n",
    "history[model_name] = compile_and_fit(argru_wx_model, weather_window)\n",
    "IPython.display.clear_output()\n",
    "\n",
    "val_performance[model_name] = argru_wx_model.evaluate(weather_window.val, verbose=0)\n",
    "test_performance[model_name] = argru_wx_model.evaluate(weather_window.test, verbose=1)\n",
    "\n",
    "model_loss(history[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_window.plot(argru_wx_model, plot_col=\"prod_wind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger AR LSTM with Weather Data\n",
    "Same inputs as previous model, more units (128 instead of 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ARLSTM 128 + wx'\n",
    "\n",
    "arlstm128_wx_model = FeedBack(units=128, out_steps=PREDICTION_STEPS)\n",
    "\n",
    "history[model_name] = compile_and_fit(arlstm128_wx_model, weather_window)\n",
    "IPython.display.clear_output()\n",
    "\n",
    "val_performance[model_name] = arlstm128_wx_model.evaluate(weather_window.val, verbose=0)\n",
    "test_performance[model_name] = arlstm128_wx_model.evaluate(weather_window.test, verbose=1)\n",
    "\n",
    "model_loss(history[model_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_window.plot(arlstm128_wx_model, plot_col=\"prod_wind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked LSTM\n",
    "- This LSTM consists of two layers witch 96 and 32 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedBackStacked(tf.keras.Model):\n",
    "    def __init__(self, units1, units2, out_steps):\n",
    "        super().__init__()\n",
    "        self.out_steps = out_steps\n",
    "        self.units1 = units1\n",
    "        self.units2 = units2\n",
    "        self.rnn_cell1 = tf.keras.layers.LSTMCell(units1)\n",
    "        self.rnn_cell2 = tf.keras.layers.LSTMCell(units2)\n",
    "        # Also wrap the LSTMCell in an RNN to simplify the `warmup` method.\n",
    "        self.rnn_layer1 = tf.keras.layers.RNN(\n",
    "            self.rnn_cell1, return_state=True, return_sequences=True)\n",
    "        self.rnn_layer2 = tf.keras.layers.RNN(\n",
    "            self.rnn_cell2, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(num_predictions)\n",
    "\n",
    "    def warmup(self, inputs_past):\n",
    "        # inputs.shape => (batch, time, features)\n",
    "        # x.shape => (batch, lstm_units)\n",
    "        x, *state1 = self.rnn_layer1(inputs_past)\n",
    "        x, *state2 = self.rnn_layer2(x)\n",
    "        # predictions.shape => (batch, features)\n",
    "        prediction = self.dense(x)\n",
    "        return prediction, state1, state2\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # unpack the past and future inputs\n",
    "        inputs_past, inputs_future = inputs\n",
    "        # Use a TensorArray to capture dynamically unrolled outputs.\n",
    "        predictions = []\n",
    "        # Past phase: Initialize the LSTM state\n",
    "        prediction, state1, state2 = self.warmup(inputs_past)\n",
    "        # Insert the first prediction.\n",
    "        predictions.append(prediction)\n",
    "        # Future phase: Run the rest of the prediction steps\n",
    "        for n in range(1, self.out_steps):\n",
    "            # Select the known input at the current time step\n",
    "            input = inputs_future[:, n-1, :]\n",
    "            # Use the last prediction as unknown input and combine it with the known input\n",
    "            # x.shape => (batch, features)\n",
    "            x = tf.concat([prediction, input], axis=1)\n",
    "            # Execute one lstm step\n",
    "            x, state1 = self.rnn_cell1(x, states=state1,\n",
    "                                     training=training)\n",
    "            x, state2 = self.rnn_cell2(x, states=state2,\n",
    "                                     training=training)\n",
    "            # Convert the lstm output to a prediction.\n",
    "            prediction = self.dense(x)\n",
    "            # Add the prediction to the output.\n",
    "            predictions.append(prediction)\n",
    "        # predictions.shape => (time, batch, features)\n",
    "        predictions = tf.stack(predictions)\n",
    "        # predictions.shape => (batch, time, features)\n",
    "        predictions = tf.transpose(predictions, [1, 0, 2])\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ARLSTM 96x32 + wx'\n",
    "\n",
    "stacked_model = FeedBackStacked(units1=96, units2=32, out_steps=PREDICTION_STEPS)\n",
    "\n",
    "history[model_name] = compile_and_fit(stacked_model, weather_window)\n",
    "IPython.display.clear_output()\n",
    "\n",
    "val_performance[model_name] = stacked_model.evaluate(weather_window.val, verbose=0)\n",
    "test_performance[model_name] = stacked_model.evaluate(weather_window.test, verbose=1)\n",
    "\n",
    "model_loss(history[model_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(test_performance))\n",
    "width = 0.3\n",
    "\n",
    "mae_index = arlstm_model.metrics_names.index('mae')\n",
    "r2_index = arlstm_model.metrics_names.index('r_square')\n",
    "\n",
    "val_r2 = [v[r2_index] for v in val_performance.values()]\n",
    "test_r2 = [v[r2_index] for v in test_performance.values()]\n",
    "\n",
    "plt.bar(x - 0.17, val_r2, width, label='Validation')\n",
    "plt.bar(x + 0.17, test_r2, width, label='Test')\n",
    "plt.xticks(ticks=x, labels=test_performance.keys(),\n",
    "           rotation=45, ha='right')\n",
    "plt.ylabel('R² score (average over all outputs)')\n",
    "_ = plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"Model on test set\":20s}    MAE     R²')\n",
    "print('-------------------- ------ ------')\n",
    "for name, value in test_performance.items():\n",
    "    print(f'{name+\":\":20s} {value[mae_index]:0.4f} {value[r2_index]:0.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "- Cross validate the model: Split the data set into $k$ subsets and train the model $k-1$ times, while taking one subset as validation set and the other subsets as training set. Reset the model after each training and compare the scores afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "# Separate the dataframe into training and test set\n",
    "n = len(df2)\n",
    "df3_train = df2[0:int(n*0.9)]\n",
    "df3_test = df2[int(n*0.9):]\n",
    "\n",
    "scaler = RobustScaler()\n",
    "# fit scaler to training data\n",
    "scaler.fit(df3_train)\n",
    "# scale all sets according to train set, preserve data frames\n",
    "df3_train = pd.DataFrame(scaler.transform(df3_train),\n",
    "                         columns=df2.columns, index=df3_train.index)\n",
    "df3_test = pd.DataFrame(scaler.transform(df3_test),\n",
    "                        columns=df2.columns, index=df3_test.index)\n",
    "\n",
    "# Define the K-fold cross validator\n",
    "tsplit = TimeSeriesSplit(n_splits=NUM_FOLDS)\n",
    "cv_val_metrics = []\n",
    "cv_test_metrics = []\n",
    "\n",
    "# K-fold cross validation model evaluation\n",
    "for fold_no, (train_index, val_index) in enumerate(tsplit.split(df3_train)):\n",
    "    df3_cv_train = df3_train.iloc[train_index]\n",
    "    df3_cv_val = df3_train.iloc[val_index]\n",
    "\n",
    "    # Create Window for current fold\n",
    "    cv_window = WindowGenerator(\n",
    "        past_width=PAST_STEPS, future_width=PREDICTION_STEPS,\n",
    "        label_columns=prediction_features,\n",
    "        precise_columns=precise_features,\n",
    "        forecast_columns=forecast_features,\n",
    "        train_df=df3_cv_train, val_df=df3_cv_val, test_df=df3_test\n",
    "    )\n",
    "\n",
    "    # Create the model to be evaluated (with freshly initialized weights)\n",
    "    cv_model = FeedBackGRU(units=32, out_steps=PREDICTION_STEPS)\n",
    "\n",
    "    # Generate a print\n",
    "    print(f'Training for fold {fold_no+1}/{NUM_FOLDS}...')\n",
    "\n",
    "    # Compile and fit model\n",
    "    compile_and_fit(cv_model, cv_window, epochs=25,\n",
    "                    stop_early=True, patience=4, verbose=0)\n",
    "\n",
    "    # Evaluate model\n",
    "    print(f'Evaluate model for fold {fold_no+1} on validation set:')\n",
    "    cv_val_metrics.append(cv_model.evaluate(cv_window.val, verbose=2))\n",
    "    print(f'Evaluate model for fold {fold_no+1} on test set:')\n",
    "    cv_test_metrics.append(cv_model.evaluate(cv_window.test, verbose=2))\n",
    "    print('------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the chosen cross validation method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for fold_no, (train_index, val_index) in enumerate(tsplit.split(df3_train)):\n",
    "    ax.scatter(\n",
    "        train_index,\n",
    "        [fold_no] * len(train_index),\n",
    "        marker=\"|\",\n",
    "        c='C0')\n",
    "    ax.scatter(\n",
    "        val_index,\n",
    "        [fold_no] * len(val_index),\n",
    "        marker=\"|\",\n",
    "        c='C3')\n",
    "# Formatting\n",
    "_ = ax.set(yticks=np.arange(NUM_FOLDS),\n",
    "       yticklabels=np.arange(NUM_FOLDS)+1,\n",
    "       xlabel='Dataset index',\n",
    "       ylabel='CV iteration',\n",
    "       title='Cross Validation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_index = cv_model.metrics_names.index('r_square')\n",
    "print('-------------------')\n",
    "print(f'Fold Val R² Test R²')\n",
    "print('---- ------ -------')\n",
    "for i, test_metric in enumerate(cv_test_metrics):\n",
    "    val_metric = cv_val_metrics[i]\n",
    "    print(f'{i+1} {val_metric[r2_index]: 9.4f} {test_metric[r2_index]: 6.4f}')\n",
    "print('---- ------ -------')\n",
    "print(f'AVG {np.average(cv_val_metrics, axis=0)[r2_index]: 7.4f} {np.average(cv_test_metrics, axis=0)[r2_index]: 6.4f}')\n",
    "print('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18e9655652794916721c836f64f0f2c40614fd3025e6f3f5246a082aecd982be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
